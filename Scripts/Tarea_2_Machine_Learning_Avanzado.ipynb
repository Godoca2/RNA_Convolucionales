{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea N°2 Machine Learning Avanzado\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://i.ibb.co/v3CvVz9/udd-short.png\" width=\"150\"/>\n",
    "    <br>\n",
    "    <strong>Universidad del Desarrollo</strong><br>\n",
    "    <em>Profesor: Tomás Fontecilla </em><br>\n",
    "\n",
    "</div>\n",
    "\n",
    "*2 de diciembre de 2024*\n",
    "\n",
    "**Nombre Estudiante(s)**:  \n",
    "\n",
    "- Julio Assmann Segura\n",
    "- César Godoy Delaigue\n",
    "- Nicolás Gonzalez Infante\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En el ámbito de la clasificación de imágenes, el uso de redes neuronales convolucionales (CNN) ha demostrado ser altamente eficiente en tareas de reconocimiento de patrones visuales. Este informe aborda la implementación de diferentes modelos de redes neuronales aplicados a la clasificación binaria de imágenes, con el objetivo de identificar características específicas en un conjunto de datos de imágenes.\n",
    "\n",
    "El trabajo se basa en el entrenamiento de modelos para una tarea de clasificación binaria utilizando un conjunto de datos organizado en carpetas para entrenamiento y prueba. Se desarrollaron y evaluaron tres modelos: dos propuestos por los miembros del grupo y un tercer modelo basado en transferencia de aprendizaje para complementar el análisis.\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Comparar el desempeño de tres modelos de aprendizaje profundo para la clasificación binaria de imágenes, evaluando métricas clave como la precisión y el f1-score, y proponer un modelo con transferencia de aprendizaje para mejorar los resultados.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodología\n",
    "\n",
    "El análisis se desarrolló siguiendo las siguientes etapas:\n",
    "\n",
    "### 1. Preparación y Preprocesamiento de los Datos\n",
    "\n",
    "- **Carga y Estandarización**:\n",
    "  Se utilizaron generadores de datos (`ImageDataGenerator`) para realizar la normalización de las imágenes al rango `[0, 1]`. Además, se implementaron técnicas de aumentación de datos, como rotaciones, zoom, y desplazamientos, para mejorar la generalización de los modelos.\n",
    "\n",
    "- **División del Conjunto de Datos**:\n",
    "  El conjunto de datos se dividió en dos partes:\n",
    "  - **Entrenamiento**: 80% de las imágenes.\n",
    "  - **Prueba**: 20% de las imágenes.\n",
    "\n",
    "### 2. Construcción e Implementación de Modelos\n",
    "\n",
    "Se desarrollaron tres arquitecturas distintas:\n",
    "\n",
    "\n",
    "#### **Modelo 1**: CNN - MLP\n",
    "\n",
    "\n",
    "#### **Modelo 2**: CNN - MLP\n",
    "\n",
    "\n",
    "\n",
    "#### **Modelo 3**: Transferencia de Aprendizaje\n",
    "- Se utilizó MobileNet preentrenado con pesos de `imagenet`.\n",
    "- Las capas convolucionales se congelaron para transferir las características aprendidas.\n",
    "- Se añadieron capas densas personalizadas con regularización (`Dropout`).\n",
    "- Arquitectura compacta y eficiente diseñada para la clasificación binaria.\n",
    "\n",
    "### 3. Optimización y Ajuste de Hiperparámetros\n",
    "\n",
    "- **Algoritmo de Optimización**: Adam, con tasas de aprendizaje ajustadas para cada modelo.\n",
    "- **Hiperparámetros Clave**:\n",
    "  - Número de capas y neuronas.\n",
    "  - Tasa de aprendizaje.\n",
    "  - Tamaño del batch y número de épocas.\n",
    "\n",
    "### 4. Evaluación y Validación\n",
    "\n",
    "- **Métricas de Evaluación**:\n",
    "  - Precisión (`accuracy`).\n",
    "  - F1-score, recall y precisión para un análisis más profundo.\n",
    "  - Validación Curva ROC - Matriz de Confución\n",
    "- **Prevención de Sobreajuste**:\n",
    "  - Uso de técnicas como early stopping y regularización.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Está habilitada la GPU? []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"¿Está habilitada la GPU?\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se detectaron GPUs disponibles.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Listar los dispositivos físicos\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"GPU detectada:\", physical_devices)\n",
    "else:\n",
    "    print(\"No se detectaron GPUs disponibles.\")\n",
    "\n",
    "# Información adicional sobre la GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(\"Detalles de la GPU:\", details)\n",
    "    except Exception as e:\n",
    "        print(\"Error al obtener detalles:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este entorno NO tiene soporte para GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Este entorno tiene soporte para GPU\")\n",
    "    print(f\"Dispositivo CUDA disponible: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Este entorno NO tiene soporte para GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Librerias***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samuelcortinhas/muffin-vs-chihuahua-image-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Directory: C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\\train\n",
      "Test Directory: C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\\test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ruta base del dataset descargado desde Kaggle\n",
    "base_dir = r\"C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\"\n",
    "\n",
    "\n",
    "# Definir las rutas para las carpetas de entrenamiento y prueba\n",
    "train_dir = os.path.join(base_dir, 'train')  # Cambia 'train' si el nombre de la carpeta es diferente\n",
    "test_dir = os.path.join(base_dir, 'test')    # Cambia 'test' si el nombre de la carpeta es diferente\n",
    "\n",
    "print(\"Train Directory:\", train_dir)\n",
    "print(\"Test Directory:\", test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Modelo 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolucion 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Directory: C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\\train\n",
      "Test Directory: C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\\test\n",
      "Found 4733 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 387ms/step - accuracy: 0.6322 - loss: 0.7764 - val_accuracy: 0.7179 - val_loss: 0.5053\n",
      "Epoch 2/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 367ms/step - accuracy: 0.7981 - loss: 0.4739 - val_accuracy: 0.8716 - val_loss: 0.3611\n",
      "Epoch 3/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 357ms/step - accuracy: 0.7743 - loss: 0.4699 - val_accuracy: 0.8674 - val_loss: 0.3294\n",
      "Epoch 4/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 365ms/step - accuracy: 0.8343 - loss: 0.4071 - val_accuracy: 0.8860 - val_loss: 0.2967\n",
      "Epoch 5/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 371ms/step - accuracy: 0.8490 - loss: 0.3711 - val_accuracy: 0.8657 - val_loss: 0.3411\n",
      "Epoch 6/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 341ms/step - accuracy: 0.8564 - loss: 0.3397 - val_accuracy: 0.8725 - val_loss: 0.3182\n",
      "Epoch 7/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 357ms/step - accuracy: 0.8577 - loss: 0.3472 - val_accuracy: 0.8910 - val_loss: 0.2710\n",
      "Epoch 8/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 384ms/step - accuracy: 0.8833 - loss: 0.3025 - val_accuracy: 0.8674 - val_loss: 0.2969\n",
      "Epoch 9/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 366ms/step - accuracy: 0.8796 - loss: 0.2865 - val_accuracy: 0.8970 - val_loss: 0.2526\n",
      "Epoch 10/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 363ms/step - accuracy: 0.8904 - loss: 0.2798 - val_accuracy: 0.8606 - val_loss: 0.3233\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - accuracy: 0.8635 - loss: 0.3130\n",
      "Test Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Ruta base del dataset descargado desde Kaggle\n",
    "base_dir = r\"C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\"\n",
    "\n",
    "\n",
    "# Definir las rutas para las carpetas de entrenamiento y prueba\n",
    "train_dir = os.path.join(base_dir, 'train')  # Cambia 'train' si el nombre de la carpeta es diferente\n",
    "test_dir = os.path.join(base_dir, 'test')    # Cambia 'test' si el nombre de la carpeta es diferente\n",
    "\n",
    "print(\"Train Directory:\", train_dir)\n",
    "print(\"Test Directory:\", test_dir)\n",
    "\n",
    "\n",
    "# Preprocesamiento de imágenes\n",
    "img_height, img_width = 150, 150\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Generadores de datos\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Construcción del modelo\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")  # Clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Evaluación del modelo\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolucion 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4733 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 588ms/step - accuracy: 0.7458 - loss: 0.5326 - val_accuracy: 0.5405 - val_loss: 0.9437\n",
      "Epoch 2/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 609ms/step - accuracy: 0.8603 - loss: 0.3401 - val_accuracy: 0.5397 - val_loss: 1.2518\n",
      "Epoch 3/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 579ms/step - accuracy: 0.8790 - loss: 0.2926 - val_accuracy: 0.8041 - val_loss: 0.4312\n",
      "Epoch 4/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 588ms/step - accuracy: 0.8946 - loss: 0.2731 - val_accuracy: 0.9113 - val_loss: 0.2445\n",
      "Epoch 5/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 580ms/step - accuracy: 0.9024 - loss: 0.2541 - val_accuracy: 0.8868 - val_loss: 0.2774\n",
      "Epoch 6/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 584ms/step - accuracy: 0.9092 - loss: 0.2346 - val_accuracy: 0.8885 - val_loss: 0.2680\n",
      "Epoch 7/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 586ms/step - accuracy: 0.8980 - loss: 0.2510 - val_accuracy: 0.8851 - val_loss: 0.2834\n",
      "Epoch 8/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 589ms/step - accuracy: 0.9072 - loss: 0.2325 - val_accuracy: 0.8910 - val_loss: 0.2563\n",
      "Epoch 9/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 585ms/step - accuracy: 0.9185 - loss: 0.2113 - val_accuracy: 0.8919 - val_loss: 0.2329\n",
      "Epoch 10/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 579ms/step - accuracy: 0.9169 - loss: 0.2049 - val_accuracy: 0.8556 - val_loss: 0.3394\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 161ms/step - accuracy: 0.8522 - loss: 0.3512\n",
      "Test Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Aumentación de datos para el entrenamiento\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalización (valores entre 0 y 1)\n",
    "    rotation_range=20,  # Rotaciones aleatorias de hasta 20 grados\n",
    "    width_shift_range=0.2,  # Desplazamiento horizontal de hasta 20%\n",
    "    height_shift_range=0.2,  # Desplazamiento vertical de hasta 20%\n",
    "    shear_range=0.2,  # Corte/distorsión\n",
    "    zoom_range=0.2,  # Zoom aleatorio\n",
    "    horizontal_flip=True,  # Inversión horizontal\n",
    "    fill_mode=\"nearest\"  # Relleno de los bordes\n",
    ")\n",
    "\n",
    "# Preprocesamiento para el conjunto de prueba (sin aumentación)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Generadores de datos\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\"  # Clasificación binaria\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Construcción del modelo\n",
    "model = Sequential([\n",
    "    # Primera capa convolucional\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 3)),\n",
    "    BatchNormalization(),  # Normaliza las activaciones\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Segunda capa convolucional\n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Tercera capa convolucional\n",
    "    Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Cuarta capa convolucional\n",
    "    Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Global Average Pooling\n",
    "    GlobalAveragePooling2D(),  # Reduce las características a un vector global\n",
    "\n",
    "    # Capa densa\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dropout(0.5),  # Regularización para evitar sobreajuste\n",
    "\n",
    "    # Capa de salida\n",
    "    Dense(1, activation=\"sigmoid\")  # Clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Tasa de aprendizaje ajustada\n",
    "    loss=\"binary_crossentropy\",  # Pérdida para clasificación binaria\n",
    "    metrics=[\"accuracy\"]  # Métrica para evaluar el rendimiento\n",
    ")\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 10  # Número de ciclos de entrenamiento\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Evaluación del modelo\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrón Multicapa (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4733 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 595ms/step - accuracy: 0.5570 - loss: 0.9133 - val_accuracy: 0.5811 - val_loss: 0.9194\n",
      "Epoch 2/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 590ms/step - accuracy: 0.5935 - loss: 0.8378 - val_accuracy: 0.6267 - val_loss: 0.7059\n",
      "Epoch 3/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 602ms/step - accuracy: 0.6287 - loss: 0.7747 - val_accuracy: 0.7103 - val_loss: 0.5484\n",
      "Epoch 4/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 619ms/step - accuracy: 0.6354 - loss: 0.7570 - val_accuracy: 0.6985 - val_loss: 0.5539\n",
      "Epoch 5/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 627ms/step - accuracy: 0.6465 - loss: 0.7362 - val_accuracy: 0.6546 - val_loss: 0.6004\n",
      "Epoch 6/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 611ms/step - accuracy: 0.6367 - loss: 0.7413 - val_accuracy: 0.7044 - val_loss: 0.5587\n",
      "Epoch 7/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 590ms/step - accuracy: 0.6436 - loss: 0.7179 - val_accuracy: 0.6883 - val_loss: 0.5942\n",
      "Epoch 8/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 591ms/step - accuracy: 0.6490 - loss: 0.7013 - val_accuracy: 0.7061 - val_loss: 0.5521\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.7135 - loss: 0.5384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocesamiento de imágenes\n",
    "\n",
    "img_height, img_width = 150, 150  # Tamaño de las imágenes (se redimensionan a 150x150)\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalización\n",
    "    rotation_range=30,  # Más rotación para aumentar la variabilidad\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.3,  # Más corte\n",
    "    zoom_range=0.3,  # Más zoom\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Generadores de datos\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Construcción del modelo Perceptrón Multicapa (MLP) Mejorado\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(img_height, img_width, 3)),  # Aplanar las imágenes\n",
    "    Dense(1024, activation=\"relu\"),  # Más neuronas para mayor capacidad\n",
    "    BatchNormalization(),            # Normalización por lotes para estabilizar el entrenamiento\n",
    "    Dropout(0.5),                    # Regularización para reducir sobreajuste\n",
    "    Dense(512, activation=\"relu\"),   # Segunda capa totalmente conectada\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation=\"relu\"),   # Capa adicional para mayor profundidad\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")   # Clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Tasa de aprendizaje más baja para mayor estabilidad\n",
    "    loss=\"binary_crossentropy\",          # Función de pérdida para clasificación binaria\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks para mejorar el entrenamiento\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Monitorea la pérdida de validación\n",
    "    patience=5,          # Detener si no mejora después de 5 épocas\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 50  # Más épocas para darle tiempo al modelo a aprender\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[early_stopping]  # Implementa parada temprana\n",
    ")\n",
    "\n",
    "# Evaluación del modelo\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save(\"muffin_vs_chihuahua_mlp_model_improved.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Modelo 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar imágenes\n",
    "def load_images_from_directory_with_tqdm(directory, target_size=(150, 150)):\n",
    "    data = []\n",
    "    for label in tqdm(os.listdir(directory), desc=\"Cargando carpetas (clases)\"):\n",
    "        label_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(label_dir):  # Verifica que sea un directorio\n",
    "            for image_file in tqdm(os.listdir(label_dir), desc=f\"Cargando imágenes de {label}\", leave=False):\n",
    "                image_path = os.path.join(label_dir, image_file)\n",
    "                data.append({'image_path': image_path, 'label': label})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando carpetas (clases): 100%|██████████| 2/2 [00:00<00:00, 69.04it/s]\n",
      "Cargando carpetas (clases): 100%|██████████| 2/2 [00:00<00:00, 111.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos de entrenamiento y prueba\n",
    "train_data = load_images_from_directory_with_tqdm(r\"C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\\train\")\n",
    "test_data = load_images_from_directory_with_tqdm(r\"C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\\test\")\n",
    "\n",
    "# Función para convertir rutas de imágenes\n",
    "def prepare_images_with_tqdm(data, target_size=(150, 150)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Cargando imágenes\", total=len(data)):\n",
    "        image = load_img(row['image_path'], target_size=target_size)  \n",
    "        image_array = img_to_array(image) / 255.0  \n",
    "        images.append(image_array)\n",
    "        labels.append(row['label'])\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando imágenes: 100%|██████████| 4733/4733 [00:15<00:00, 298.29it/s]\n",
      "Cargando imágenes: 100%|██████████| 1184/1184 [00:03<00:00, 316.24it/s]\n",
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando el modelo MLP...\n",
      "Epoch 1/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 113ms/step - accuracy: 0.4916 - loss: 13.1328 - val_accuracy: 0.5405 - val_loss: 0.6905\n",
      "Epoch 2/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 111ms/step - accuracy: 0.5452 - loss: 0.6919 - val_accuracy: 0.5405 - val_loss: 0.6909\n",
      "Epoch 3/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 113ms/step - accuracy: 0.5512 - loss: 0.6901 - val_accuracy: 0.5405 - val_loss: 0.6902\n",
      "Epoch 4/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 112ms/step - accuracy: 0.5331 - loss: 0.7237 - val_accuracy: 0.5405 - val_loss: 0.6900\n",
      "Epoch 5/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 113ms/step - accuracy: 0.5352 - loss: 0.6907 - val_accuracy: 0.5405 - val_loss: 0.6899\n",
      "Epoch 6/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 115ms/step - accuracy: 0.5359 - loss: 0.6906 - val_accuracy: 0.5405 - val_loss: 0.6899\n",
      "Epoch 7/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 111ms/step - accuracy: 0.5399 - loss: 0.6902 - val_accuracy: 0.5405 - val_loss: 0.6898\n",
      "Epoch 8/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 111ms/step - accuracy: 0.5426 - loss: 0.6899 - val_accuracy: 0.5405 - val_loss: 0.6898\n",
      "Epoch 9/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 111ms/step - accuracy: 0.5408 - loss: 0.6898 - val_accuracy: 0.5405 - val_loss: 0.6898\n",
      "Epoch 10/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 111ms/step - accuracy: 0.5401 - loss: 0.6900 - val_accuracy: 0.5405 - val_loss: 0.6898\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8584 - loss: 0.6400\n",
      "\n",
      "Resultados del modelo MLP:\n",
      "Pérdida: 0.6898, Precisión: 54.05%\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar datos\n",
    "X_train, y_train = prepare_images_with_tqdm(train_data)\n",
    "X_test, y_test = prepare_images_with_tqdm(test_data)\n",
    "\n",
    "# Convertir etiquetas a valores binarios (0 y 1)\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Modelo de Perceptrón Multicapa\n",
    "mlp_model = Sequential([\n",
    "    Flatten(input_shape=(150, 150, 3)),  \n",
    "    Dense(256, activation='relu'),  \n",
    "    Dropout(0.5),  \n",
    "    Dense(128, activation='relu'),  \n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  #\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento del MLP\n",
    "print(\"\\nEntrenando el modelo MLP...\")\n",
    "mlp_history = mlp_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "# Evaluación del MLP\n",
    "loss, accuracy = mlp_model.evaluate(X_test, y_test)\n",
    "print(f\"\\nResultados del modelo MLP:\")\n",
    "print(f\"Pérdida: {loss:.4f}, Precisión: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando el modelo CNN...\n",
      "Epoch 1/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 186ms/step - accuracy: 0.6388 - loss: 0.6358 - val_accuracy: 0.8429 - val_loss: 0.3610\n",
      "Epoch 2/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 184ms/step - accuracy: 0.8369 - loss: 0.3917 - val_accuracy: 0.8387 - val_loss: 0.3563\n",
      "Epoch 3/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 184ms/step - accuracy: 0.8536 - loss: 0.3486 - val_accuracy: 0.8834 - val_loss: 0.2748\n",
      "Epoch 4/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 183ms/step - accuracy: 0.9005 - loss: 0.2532 - val_accuracy: 0.8666 - val_loss: 0.3106\n",
      "Epoch 5/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 184ms/step - accuracy: 0.9146 - loss: 0.2164 - val_accuracy: 0.8995 - val_loss: 0.2383\n",
      "Epoch 6/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 184ms/step - accuracy: 0.9298 - loss: 0.1945 - val_accuracy: 0.9088 - val_loss: 0.2132\n",
      "Epoch 7/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 187ms/step - accuracy: 0.9445 - loss: 0.1478 - val_accuracy: 0.9037 - val_loss: 0.2480\n",
      "Epoch 8/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 185ms/step - accuracy: 0.9534 - loss: 0.1212 - val_accuracy: 0.8961 - val_loss: 0.2789\n",
      "Epoch 9/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 186ms/step - accuracy: 0.9636 - loss: 0.0952 - val_accuracy: 0.9181 - val_loss: 0.2480\n",
      "Epoch 10/10\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 185ms/step - accuracy: 0.9775 - loss: 0.0730 - val_accuracy: 0.9198 - val_loss: 0.2579\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9216 - loss: 0.2463\n",
      "\n",
      "Resultados del modelo CNN:\n",
      "Pérdida: 0.2579, Precisión: 91.98%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Modelo CNN\n",
    "cnn_model = Sequential([\n",
    "    # Primera capa convolucional + max pooling\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),  \n",
    "    MaxPooling2D(pool_size=(2, 2)), \n",
    "\n",
    "    # Segunda capa convolucional + max pooling\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Tercera capa convolucional + max pooling\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Aplanar y capas densas\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),  \n",
    "    Dropout(0.5),  \n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento del modelo CNN\n",
    "print(\"\\nEntrenando el modelo CNN...\")\n",
    "cnn_history = cnn_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "# Evaluación del modelo CNN\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f\"\\nResultados del modelo CNN:\")\n",
    "print(f\"Pérdida: {cnn_loss:.4f}, Precisión: {cnn_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación entre MLP y CNN:\n",
      "MLP - Precisión: 54.05%\n",
      "CNN - Precisión: 91.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparación entre MLP y CNN:\")\n",
    "print(f\"MLP - Precisión: {accuracy * 100:.2f}%\")\n",
    "print(f\"CNN - Precisión: {cnn_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando el modelo CNN Mejorado...\n",
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 495ms/step - accuracy: 0.7036 - loss: 1.5623 - val_accuracy: 0.5405 - val_loss: 2.6217\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 492ms/step - accuracy: 0.8235 - loss: 0.5486 - val_accuracy: 0.5405 - val_loss: 1.6550\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 492ms/step - accuracy: 0.8635 - loss: 0.3922 - val_accuracy: 0.6292 - val_loss: 1.1377\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 496ms/step - accuracy: 0.8818 - loss: 0.3441 - val_accuracy: 0.9096 - val_loss: 0.2571\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 492ms/step - accuracy: 0.9095 - loss: 0.2516 - val_accuracy: 0.8699 - val_loss: 0.3294\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 491ms/step - accuracy: 0.9154 - loss: 0.2314 - val_accuracy: 0.8032 - val_loss: 0.4861\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 493ms/step - accuracy: 0.9215 - loss: 0.2124 - val_accuracy: 0.9299 - val_loss: 0.1945\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 490ms/step - accuracy: 0.9342 - loss: 0.1775 - val_accuracy: 0.9003 - val_loss: 0.2560\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 491ms/step - accuracy: 0.9426 - loss: 0.1557 - val_accuracy: 0.8927 - val_loss: 0.2658\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 491ms/step - accuracy: 0.9422 - loss: 0.1476 - val_accuracy: 0.9257 - val_loss: 0.1928\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 493ms/step - accuracy: 0.9552 - loss: 0.1192 - val_accuracy: 0.9088 - val_loss: 0.2800\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 493ms/step - accuracy: 0.9598 - loss: 0.1114 - val_accuracy: 0.9257 - val_loss: 0.1838\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 489ms/step - accuracy: 0.9656 - loss: 0.0874 - val_accuracy: 0.9299 - val_loss: 0.1917\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 489ms/step - accuracy: 0.9709 - loss: 0.0695 - val_accuracy: 0.9400 - val_loss: 0.2112\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 487ms/step - accuracy: 0.9812 - loss: 0.0555 - val_accuracy: 0.9417 - val_loss: 0.1673\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 499ms/step - accuracy: 0.9822 - loss: 0.0526 - val_accuracy: 0.8750 - val_loss: 0.4368\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 500ms/step - accuracy: 0.9833 - loss: 0.0419 - val_accuracy: 0.9350 - val_loss: 0.2212\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 519ms/step - accuracy: 0.9836 - loss: 0.0529 - val_accuracy: 0.9324 - val_loss: 0.1728\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 517ms/step - accuracy: 0.9849 - loss: 0.0418 - val_accuracy: 0.9350 - val_loss: 0.1828\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 500ms/step - accuracy: 0.9898 - loss: 0.0280 - val_accuracy: 0.9341 - val_loss: 0.1916\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.9341 - loss: 0.1926\n",
      "\n",
      "Resultados del modelo CNN Mejorado:\n",
      "Pérdida: 0.1916, Precisión: 93.41%\n"
     ]
    }
   ],
   "source": [
    "# Modelo CNN Mejorado\n",
    "cnn_model_mejorado = Sequential([\n",
    "    # Primera capa convolucional + batch normalization + max pooling\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Segunda capa convolucional + batch normalization + max pooling\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Tercera capa convolucional + batch normalization + max pooling\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Cuarta capa convolucional + batch normalization + max pooling\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Aplanar y capas densas\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),  # Capa densa grande\n",
    "    Dropout(0.5),  # Regularización\n",
    "    Dense(256, activation='relu'),  # Capa densa adicional\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Capa de salida (clasificación binaria)\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "cnn_model_mejorado.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),  # Learning rate más bajo\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo CNN Mejorado\n",
    "print(\"\\nEntrenando el modelo CNN Mejorado...\")\n",
    "cnn_history_mejorado = cnn_model_mejorado.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # Más épocas para mejorar el aprendizaje\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluación del modelo CNN Mejorado\n",
    "cnn_loss_mejorado, cnn_accuracy_mejorado = cnn_model_mejorado.evaluate(X_test, y_test)\n",
    "print(f\"\\nResultados del modelo CNN Mejorado:\")\n",
    "print(f\"Pérdida: {cnn_loss_mejorado:.4f}, Precisión: {cnn_accuracy_mejorado * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step\n",
      "Total de imágenes clasificadas como 'chihuahua': 550 de 1184 imágenes de prueba.\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   chihuahua       0.94      0.93      0.94       640\n",
      "      muffin       0.92      0.93      0.93       544\n",
      "\n",
      "    accuracy                           0.93      1184\n",
      "   macro avg       0.93      0.93      0.93      1184\n",
      "weighted avg       0.93      0.93      0.93      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicciones para el conjunto de prueba\n",
    "y_pred = cnn_model_mejorado.predict(X_test)\n",
    "\n",
    "# Convertir las probabilidades a clases binarias (0 = muffin, 1 = chihuahua)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "# Contar cuántas imágenes fueron clasificadas como \"chihuahua\"\n",
    "chihuahua_predicted_count = (y_pred_binary == 1).sum()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Total de imágenes clasificadas como 'chihuahua': {chihuahua_predicted_count} de {len(y_test)} imágenes de prueba.\")\n",
    "\n",
    "# Evaluar precisión del modelo\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, y_pred_binary, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Modelo 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4733 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cgodo\\AppData\\Local\\Temp\\ipykernel_35516\\3751288851.py:49: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenet_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenet_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m3,228,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,491,521</span> (13.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,491,521\u001b[0m (13.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,657</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,657\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,864</span> (12.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,228,864\u001b[0m (12.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Entorno_Desarrollo_UDD\\Machine_Learning_Avanzado\\entorno_machine\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 331ms/step - accuracy: 0.8149 - loss: 0.4345 - val_accuracy: 0.9814 - val_loss: 0.0449\n",
      "Epoch 2/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 335ms/step - accuracy: 0.9619 - loss: 0.0943 - val_accuracy: 0.9873 - val_loss: 0.0342\n",
      "Epoch 3/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 332ms/step - accuracy: 0.9712 - loss: 0.0801 - val_accuracy: 0.9848 - val_loss: 0.0323\n",
      "Epoch 4/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 335ms/step - accuracy: 0.9727 - loss: 0.0813 - val_accuracy: 0.9873 - val_loss: 0.0271\n",
      "Epoch 5/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 333ms/step - accuracy: 0.9812 - loss: 0.0505 - val_accuracy: 0.9890 - val_loss: 0.0260\n",
      "Epoch 6/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 332ms/step - accuracy: 0.9804 - loss: 0.0543 - val_accuracy: 0.9899 - val_loss: 0.0245\n",
      "Epoch 7/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 329ms/step - accuracy: 0.9828 - loss: 0.0446 - val_accuracy: 0.9865 - val_loss: 0.0250\n",
      "Epoch 8/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 323ms/step - accuracy: 0.9838 - loss: 0.0394 - val_accuracy: 0.9907 - val_loss: 0.0244\n",
      "Epoch 9/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 320ms/step - accuracy: 0.9816 - loss: 0.0494 - val_accuracy: 0.9907 - val_loss: 0.0217\n",
      "Epoch 10/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 329ms/step - accuracy: 0.9870 - loss: 0.0357 - val_accuracy: 0.9890 - val_loss: 0.0227\n",
      "Epoch 11/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 325ms/step - accuracy: 0.9875 - loss: 0.0370 - val_accuracy: 0.9916 - val_loss: 0.0218\n",
      "Epoch 12/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 318ms/step - accuracy: 0.9845 - loss: 0.0388 - val_accuracy: 0.9941 - val_loss: 0.0198\n",
      "Epoch 13/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 322ms/step - accuracy: 0.9867 - loss: 0.0333 - val_accuracy: 0.9907 - val_loss: 0.0198\n",
      "Epoch 14/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 316ms/step - accuracy: 0.9916 - loss: 0.0289 - val_accuracy: 0.9907 - val_loss: 0.0211\n",
      "Epoch 15/15\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 319ms/step - accuracy: 0.9881 - loss: 0.0304 - val_accuracy: 0.9907 - val_loss: 0.0186\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 190ms/step - accuracy: 0.9918 - loss: 0.0192\n",
      "Pérdida en prueba: 0.0186\n",
      "Precisión en prueba: 0.9907\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Rutas actualizadas del dataset\n",
    "data_dir = r\"C:\\Users\\cgodo\\.cache\\kagglehub\\datasets\\samuelcortinhas\\muffin-vs-chihuahua-image-classification\\versions\\2\"\n",
    "train_dir = f\"{data_dir}\\\\train\"\n",
    "test_dir = f\"{data_dir}\\\\test\"\n",
    "\n",
    "# Parámetros generales\n",
    "input_shape = (150, 150, 3)\n",
    "batch_size = 32\n",
    "\n",
    "# Preprocesamiento y aumentación de imágenes\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalización\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Generadores de datos\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Definir el modelo con transferencia de aprendizaje\n",
    "def create_transfer_learning_model(input_shape=(150, 150, 3), learning_rate=1e-4):\n",
    "    # Cargar el modelo base MobileNet con pesos preentrenados\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Congelar las capas del modelo base\n",
    "\n",
    "    # Construir el modelo\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Reduce las características para conectarlas a capas densas\n",
    "        Dense(256, activation='relu'),  # Capa densa con 256 neuronas\n",
    "        Dropout(0.5),  # Regularización para evitar sobreajuste\n",
    "        Dense(1, activation='sigmoid')  # Salida para clasificación binaria\n",
    "    ])\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Crear el modelo\n",
    "transfer_learning_model = create_transfer_learning_model()\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "transfer_learning_model.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = transfer_learning_model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,  # Ajustar el número de épocas según necesidad\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_loss, test_accuracy = transfer_learning_model.evaluate(test_generator)\n",
    "print(f\"Pérdida en prueba: {test_loss:.4f}\")\n",
    "print(f\"Precisión en prueba: {test_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de Modelos: Muffin vs Chihuahua\n",
    "\n",
    "#### 1. Arquitectura\n",
    "| Aspecto                | Modelo 1                      | Modelo 2                         | Modelo 3                      |\n",
    "|------------------------|-------------------------------|----------------------------------|-------------------------------|\n",
    "| **Capas CNN**          |                               | 4 (32, 64, 128, 256 filtros)     | Preentrenadas en MobileNet    |\n",
    "| **Pooling**            |                               | MaxPooling (4 capas)             | Global Average Pooling        |\n",
    "| **Capas Densas**       | 4 (1024, 512, 256, 1 neurona) | 2 (256 y 1 neurona)              | 1 (256 neuronas)              |\n",
    "| **Regularización**     | Dropout (0.5)                 | Dropout (0.5) + BatchNorm        | Dropout (0.5)                 |\n",
    "| **Entrenamiento**      | 10 épocas                     | 20 épocas                        | 15 épocas                     |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Resultados por Categoría\n",
    "| Métrica                       | Modelo 1                     | Modelo 2                     | Modelo 3                     |\n",
    "|--------------------------------------------------------------|------------------------------|------------------------------|\n",
    "| **Precisión (Entrenamiento)** |0                             |0                             |0                             |\n",
    "| **Precisión (Validación)**    |0                             |0                             |0                             |\n",
    "| **Precisión (Prueba)**        |0                             |0                             |0                             |\n",
    "| **Pérdida (Entrenamiento)**   |0                             |0                             |0                             |\n",
    "| **Pérdida (Validación)**      |0                             |0                             |0                             |\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Conclusión\n",
    "- **Mejor Desempeño:** El modelo de **Transfer Learning** basado en MobileNet demostró el mejor rendimiento con una precisión del 99.18% en el conjunto de prueba y una pérdida mínima de 0.0186 en validación.\n",
    "- **Generalización Superior:** Gracias al uso de pesos preentrenados, el modelo mostró una capacidad de generalización superior frente a los modelos básicos y mejorados, logrando resultados consistentes en todas las etapas del entrenamiento.\n",
    "- **Recomendación:** Para problemas similares con datos limitados, Transfer Learning es una solución óptima por su eficiencia y capacidad de generalización. Sin embargo, es importante considerar el costo computacional frente a modelos más simples como CNN y MLP.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
